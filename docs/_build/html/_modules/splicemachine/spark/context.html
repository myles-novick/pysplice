
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>splicemachine.spark.context &#8212; Splice MLManager  documentation</title>
    
  <link href="../../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx_tabs/semantic-ui-2.4.1/segment.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx_tabs/semantic-ui-2.4.1/menu.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx_tabs/semantic-ui-2.4.1/tab.min.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx_tabs/tabs.css" />
    
  <link rel="preload" as="script" href="../../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/language_data.js"></script>
    <script src="../../../_static/sphinx_tabs/semantic-ui-2.4.1/tab.min.js"></script>
    <script src="../../../_static/sphinx_tabs/tabs.js"></script>
    <script src="../../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
      
      <h1 class="site-logo" id="site-title">Splice MLManager  documentation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Contents:
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../getting-started.html">
   Getting Started
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../splicemachine.html">
   Splicemachine package
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../splicemachine.spark.html">
     splicemachine.spark package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../splicemachine.mlflow_support.html">
     splicemachine.mlflow_support package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../splicemachine.features.html">
     splicemachine.features package
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../splicemachine.notebook.html">
     splicemachine.notebook module
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../splicemachine.stats.html">
     splicemachine.stats module
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <h1>Source code for splicemachine.spark.context</h1><div class="highlight"><pre>
<span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Copyright 2021 Splice Machine, Inc.</span>

<span class="sd">Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="sd">you may not use this file except in compliance with the License.</span>
<span class="sd">You may obtain a copy of the License at</span>

<span class="sd">    http://www.apache.org/licenses/LICENSE-2.0</span>

<span class="sd">Unless required by applicable law or agreed to in writing, software</span>
<span class="sd">distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="sd">WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="sd">See the License for the specific language governing permissions and</span>
<span class="sd">limitations under the License.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">string</span> <span class="kn">import</span> <span class="n">punctuation</span> <span class="k">as</span> <span class="n">bad_chars</span>

<span class="kn">from</span> <span class="nn">py4j.java_gateway</span> <span class="kn">import</span> <span class="n">java_import</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">DataFrame</span>
<span class="kn">from</span> <span class="nn">pyspark.sql.types</span> <span class="kn">import</span> <span class="n">_parse_datatype_json_string</span><span class="p">,</span> <span class="n">StringType</span>

<span class="kn">from</span> <span class="nn">splicemachine.spark.constants</span> <span class="kn">import</span> <span class="n">CONVERSIONS</span>
<span class="kn">from</span> <span class="nn">splicemachine</span> <span class="kn">import</span> <span class="n">SpliceMachineException</span>


<div class="viewcode-block" id="PySpliceContext"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext">[docs]</a><span class="k">class</span> <span class="nc">PySpliceContext</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class implements a SpliceMachineContext object (similar to the SparkContext object)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_spliceSparkPackagesName</span> <span class="o">=</span> <span class="s2">&quot;com.splicemachine.spark.splicemachine.*&quot;</span>

    <span class="k">def</span> <span class="nf">_splicemachineContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">com</span><span class="o">.</span><span class="n">splicemachine</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">splicemachine</span><span class="o">.</span><span class="n">SplicemachineContext</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jdbcurl</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparkSession</span><span class="p">,</span> <span class="n">JDBC_URL</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">_unit_testing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param JDBC_URL: (string) The JDBC URL Connection String for your Splice Machine Cluster</span>
<span class="sd">        :param sparkSession: (sparkContext) A SparkSession object for talking to Spark</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">JDBC_URL</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">jdbcurl</span> <span class="o">=</span> <span class="n">JDBC_URL</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">jdbcurl</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;BEAKERX_SQL_DEFAULT_JDBC&#39;</span><span class="p">]</span>
            <span class="k">except</span> <span class="ne">KeyError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                    <span class="s2">&quot;Could not locate JDBC URL. If you are not running on the cloud service,&quot;</span>
                    <span class="s2">&quot;please specify the JDBC_URL=&lt;some url&gt; keyword argument in the constructor&quot;</span>
                <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_unit_testing</span> <span class="o">=</span> <span class="n">_unit_testing</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">_unit_testing</span><span class="p">:</span>  <span class="c1"># Private Internal Argument to Override Using JVM</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span> <span class="o">=</span> <span class="n">sparkSession</span><span class="o">.</span><span class="n">_wrapped</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span> <span class="o">=</span> <span class="n">sparkSession</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span><span class="o">.</span><span class="n">_sc</span><span class="o">.</span><span class="n">_jvm</span>
            <span class="n">java_import</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_spliceSparkPackagesName</span><span class="p">)</span>
            <span class="n">java_import</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s2">&quot;org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions&quot;</span><span class="p">)</span>
            <span class="n">java_import</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s2">&quot;org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils&quot;</span><span class="p">)</span>
            <span class="n">java_import</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s2">&quot;scala.collection.JavaConverters._&quot;</span><span class="p">)</span>
            <span class="n">java_import</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s2">&quot;com.splicemachine.derby.impl.*&quot;</span><span class="p">)</span>
            <span class="n">java_import</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="p">,</span> <span class="s1">&#39;org.apache.spark.api.python.PythonUtils&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">com</span><span class="o">.</span><span class="n">splicemachine</span><span class="o">.</span><span class="n">derby</span><span class="o">.</span><span class="n">impl</span><span class="o">.</span><span class="n">SpliceSpark</span><span class="o">.</span><span class="n">setContext</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span><span class="o">.</span><span class="n">_jsc</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_splicemachineContext</span><span class="p">()</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="kn">from</span> <span class="nn">.tests.mocked</span> <span class="kn">import</span> <span class="n">MockedScalaContext</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span> <span class="o">=</span> <span class="n">sparkSession</span><span class="o">.</span><span class="n">_wrapped</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span> <span class="o">=</span> <span class="n">sparkSession</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">MockedScalaContext</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">jdbcurl</span><span class="p">)</span>

<div class="viewcode-block" id="PySpliceContext.columnNamesCaseSensitive"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.columnNamesCaseSensitive">[docs]</a>    <span class="k">def</span> <span class="nf">columnNamesCaseSensitive</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">caseSensitive</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sets whether column names should be treated as case sensitive.</span>

<span class="sd">        :param caseSensitive: (boolean) True for case sensitive, False for not case sensitive</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">columnNamesCaseSensitive</span><span class="p">(</span><span class="n">caseSensitive</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.toUpper"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.toUpper">[docs]</a>    <span class="k">def</span> <span class="nf">toUpper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a dataframe with all of the columns in uppercase</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe to convert to uppercase</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
            <span class="n">s</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="c1"># You need to re-generate the dataframe for the capital letters to take effect</span>
        <span class="k">return</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.toLower"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.toLower">[docs]</a>    <span class="k">def</span> <span class="nf">toLower</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a dataframe with all of the columns in lowercase</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe to convert to lowercase</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
            <span class="n">s</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="c1"># You need to re-generate the dataframe for the capital letters to take effect</span>
        <span class="k">return</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">schema</span><span class="p">)</span></div>


<div class="viewcode-block" id="PySpliceContext.replaceDataframeSchema"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.replaceDataframeSchema">[docs]</a>    <span class="k">def</span> <span class="nf">replaceDataframeSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns a dataframe with all column names replaced with the proper string case from the DB table</span>

<span class="sd">        :param dataframe: (Dataframe) A dataframe with column names to convert</span>
<span class="sd">        :param schema_table_name: (str) The schema.table with the correct column cases to pull from the database</span>
<span class="sd">        :return: (DataFrame) A Spark DataFrame with the replaced schema</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">schema</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getSchema</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">)</span>
        <span class="c1"># Fastest way to replace the column case if changed</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">rdd</span><span class="o">.</span><span class="n">toDF</span><span class="p">(</span><span class="n">schema</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dataframe</span></div>

<div class="viewcode-block" id="PySpliceContext.fileToTable"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.fileToTable">[docs]</a>    <span class="k">def</span> <span class="nf">fileToTable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">file_path</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">primary_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">drop_table</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">pandas_args</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a file from the local filesystem or from a remote location and create a new table</span>
<span class="sd">        (or recreate an existing table), and load the data from the file into the new table. Any file_path that can be</span>
<span class="sd">        read by pandas should work here.</span>

<span class="sd">        :param file_path: The local file to load</span>
<span class="sd">        :param schema_table_name: The schema.table name</span>
<span class="sd">        :param primary_keys: List[str] of primary keys for the table. Default None</span>
<span class="sd">        :param drop_table: Whether or not to drop the table. If this is False and the table already exists, the</span>
<span class="sd">            function will fail. Default False</span>
<span class="sd">        :param pandas_args: Extra parameters to be passed into the pd.read_csv function. Any parameters accepted</span>
<span class="sd">            in pd.read_csv will work here</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
        <span class="n">pdf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="o">**</span><span class="n">pandas_args</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pandasToSpark</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">createTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">primary_keys</span><span class="o">=</span><span class="n">primary_keys</span><span class="p">,</span> <span class="n">drop_table</span><span class="o">=</span><span class="n">drop_table</span><span class="p">,</span> <span class="n">to_upper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">to_upper</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.pandasToSpark"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.pandasToSpark">[docs]</a>    <span class="k">def</span> <span class="nf">pandasToSpark</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pdf</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert a Pandas DF to Spark, and try to manage NANs from Pandas in case of failure. Spark cannot handle</span>
<span class="sd">        Pandas NAN existing in String columns (as it considers it NaN Number ironically), so we replace the occurances</span>
<span class="sd">        with a temporary value and then convert it back to null after it becomes a Spark DF</span>

<span class="sd">        :param pdf: The Pandas dataframe</span>
<span class="sd">        :return: The Spark DF</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span> <span class="c1"># Try to create the dataframe as it exists</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">TypeError</span><span class="p">:</span>
            <span class="n">p_df</span> <span class="o">=</span> <span class="n">pdf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
            <span class="c1"># This means there was an NaN conversion error</span>
            <span class="kn">from</span> <span class="nn">pyspark.sql.functions</span> <span class="kn">import</span> <span class="n">udf</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">p_df</span><span class="o">.</span><span class="n">columns</span><span class="p">:</span> <span class="c1"># Replace non numeric/time columns with a custom null value</span>
                <span class="k">if</span> <span class="n">p_df</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;int64&#39;</span><span class="p">,</span><span class="s1">&#39;float64&#39;</span><span class="p">,</span> <span class="s1">&#39;datetime64[ns]&#39;</span><span class="p">):</span>
                    <span class="n">p_df</span><span class="p">[</span><span class="n">c</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="s1">&#39;Splice_Temp_NA&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">spark_df</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">p_df</span><span class="p">)</span>
            <span class="c1"># Convert that custom null value back to null after converting to a spark dataframe</span>
            <span class="n">null_replace_udf</span> <span class="o">=</span> <span class="n">udf</span><span class="p">(</span><span class="k">lambda</span> <span class="n">name</span><span class="p">:</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;Splice_Temp_NA&quot;</span> <span class="k">else</span> <span class="n">name</span><span class="p">,</span> <span class="n">StringType</span><span class="p">())</span>
            <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">field</span><span class="o">.</span><span class="n">dataType</span><span class="o">==</span><span class="n">StringType</span><span class="p">():</span>
                    <span class="n">spark_df</span> <span class="o">=</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">withColumn</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">null_replace_udf</span><span class="p">(</span><span class="n">spark_df</span><span class="p">[</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">]))</span>
                <span class="n">spark_df</span> <span class="o">=</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">withColumnRenamed</span><span class="p">(</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;[&#39;</span><span class="o">+</span><span class="n">bad_chars</span><span class="o">+</span><span class="s1">&#39; ]&#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">,</span><span class="n">field</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
            <span class="c1"># Replace NaN numeric columns with null</span>
            <span class="n">spark_df</span> <span class="o">=</span> <span class="n">spark_df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;nan&#39;</span><span class="p">),</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">spark_df</span></div>


<div class="viewcode-block" id="PySpliceContext.getConnection"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.getConnection">[docs]</a>    <span class="k">def</span> <span class="nf">getConnection</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a connection to the database</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">getConnection</span><span class="p">()</span></div>

<div class="viewcode-block" id="PySpliceContext.tableExists"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.tableExists">[docs]</a>    <span class="k">def</span> <span class="nf">tableExists</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_and_or_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check whether or not a table exists</span>

<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                splice.tableExists(&#39;schemaName.tableName&#39;)\n</span>
<span class="sd">                # or\n</span>
<span class="sd">                splice.tableExists(&#39;schemaName&#39;, &#39;tableName&#39;)</span>

<span class="sd">        :param schema_and_or_table_name: (str) Pass the schema name in this param when passing the table_name param,</span>
<span class="sd">          or pass schemaName.tableName in this param without passing the table_name param</span>
<span class="sd">        :param table_name: (optional) (str) Table Name, used when schema_and_or_table_name contains only the schema name</span>
<span class="sd">        :return: (bool) whether or not the table exists</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">table_name</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">tableExists</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">tableExists</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.dropTable"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.dropTable">[docs]</a>    <span class="k">def</span> <span class="nf">dropTable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_and_or_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Drop a specified table.</span>

<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                splice.dropTable(&#39;schemaName.tableName&#39;) \n</span>
<span class="sd">                # or\n</span>
<span class="sd">                splice.dropTable(&#39;schemaName&#39;, &#39;tableName&#39;)</span>

<span class="sd">        :param schema_and_or_table_name: (str) Pass the schema name in this param when passing the table_name param,</span>
<span class="sd">          or pass schemaName.tableName in this param without passing the table_name param</span>
<span class="sd">        :param table_name: (optional) (str) Table Name, used when schema_and_or_table_name contains only the schema name</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">table_name</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">dropTable</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">dropTable</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.df"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.df">[docs]</a>    <span class="k">def</span> <span class="nf">df</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sql</span><span class="p">,</span> <span class="n">to_lower</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a Spark Dataframe from the results of a Splice Machine SQL Query</span>

<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                df = splice.df(&#39;SELECT * FROM MYSCHEMA.TABLE1 WHERE COL2 &gt; 3&#39;)</span>

<span class="sd">        :param sql: (str) SQL Query (eg. SELECT * FROM table1 WHERE col2 &gt; 3)</span>
<span class="sd">        :param to_lower: Whether or not to convert column names from the dataframe to lowercase</span>
<span class="sd">        :return: (Dataframe) A Spark DataFrame containing the results</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">df</span><span class="p">(</span><span class="n">sql</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">toLower</span><span class="p">(</span><span class="n">df</span><span class="p">)</span> <span class="k">if</span> <span class="n">to_lower</span> <span class="k">else</span> <span class="n">df</span></div>

<div class="viewcode-block" id="PySpliceContext.insert"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.insert">[docs]</a>    <span class="k">def</span> <span class="nf">insert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">to_upper</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">create_table</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Insert a dataframe into a table (schema.table).</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to insert</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to insert the DF</span>
<span class="sd">        :param to_upper: (bool) If the dataframe columns should be converted to uppercase before table creation</span>
<span class="sd">                            If False, the table will be created with lower case columns. [Default True]</span>
<span class="sd">        :param create_table: If the table does not exists at the time of the call, the table will first be created</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">to_upper</span><span class="p">:</span>
            <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toUpper</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">tableExists</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">create_table</span><span class="p">:</span>
                <span class="k">raise</span> <span class="n">SpliceMachineException</span><span class="p">(</span><span class="s2">&quot;Table does not exist. Create the table first or set create_table=True &quot;</span>
                                             <span class="s2">&quot;in this function, or call createAndInsertTable&quot;</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Table does not yet exist, creating table... &#39;</span><span class="p">,</span><span class="n">end</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">createTable</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">to_upper</span><span class="o">=</span><span class="n">to_upper</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Done.&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.insertWithStatus"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.insertWithStatus">[docs]</a>    <span class="k">def</span> <span class="nf">insertWithStatus</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">statusDirectory</span><span class="p">,</span> <span class="n">badRecordsAllowed</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Insert a dataframe into a table (schema.table) while tracking and limiting records that fail to insert.</span>
<span class="sd">        The status directory and number of badRecordsAllowed allow for duplicate primary keys to be</span>
<span class="sd">        written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written</span>
<span class="sd">        to the status directory.</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to insert</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to insert the dataframe</span>
<span class="sd">        :param statusDirectory: (str) The status directory where bad records file will be created</span>
<span class="sd">        :param badRecordsAllowed: (int) The number of bad records are allowed. -1 for unlimited</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replaceDataframeSchema</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">statusDirectory</span><span class="p">,</span> <span class="n">badRecordsAllowed</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.insertRdd"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.insertRdd">[docs]</a>    <span class="k">def</span> <span class="nf">insertRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Insert an rdd into a table (schema.table)</span>

<span class="sd">        :param rdd: (RDD) The RDD you would like to insert</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to insert the RDD</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.insertRddWithStatus"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.insertRddWithStatus">[docs]</a>    <span class="k">def</span> <span class="nf">insertRddWithStatus</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">statusDirectory</span><span class="p">,</span> <span class="n">badRecordsAllowed</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Insert an rdd into a table (schema.table) while tracking and limiting records that fail to insert. \</span>
<span class="sd">        The status directory and number of badRecordsAllowed allow for duplicate primary keys to be \</span>
<span class="sd">        written to a bad records file.  If badRecordsAllowed is set to -1, all bad records will be written \</span>
<span class="sd">        to the status directory.</span>

<span class="sd">        :param rdd: (RDD) The RDD you would like to insert</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to insert the dataframe</span>
<span class="sd">        :param statusDirectory: (str) The status directory where bad records file will be created</span>
<span class="sd">        :param badRecordsAllowed: (int) The number of bad records are allowed. -1 for unlimited</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">insertWithStatus</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span><span class="p">,</span>
            <span class="n">statusDirectory</span><span class="p">,</span>
            <span class="n">badRecordsAllowed</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.upsert"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.upsert">[docs]</a>    <span class="k">def</span> <span class="nf">upsert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Upsert the data from a dataframe into a table (schema.table).</span>
<span class="sd">        If triggers fail when calling upsert, use the mergeInto function instead of upsert.</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to upsert</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to upsert the RDD</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># make sure column names are in the correct case</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replaceDataframeSchema</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">upsert</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.upsertWithRdd"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.upsertWithRdd">[docs]</a>    <span class="k">def</span> <span class="nf">upsertWithRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Upsert the data from an RDD into a table (schema.table).</span>
<span class="sd">        If triggers fail when calling upsertWithRdd, use the mergeIntoWithRdd function instead of upsertWithRdd.</span>

<span class="sd">        :param rdd: (RDD) The RDD you would like to upsert</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to upsert the RDD</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">upsert</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.mergeInto"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.mergeInto">[docs]</a>    <span class="k">def</span> <span class="nf">mergeInto</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Rows in the dataframe whose primary key is not in schemaTableName will be inserted into the table;</span>
<span class="sd">        rows in the dataframe whose primary key is in schemaTableName will be used to update the table.</span>

<span class="sd">        This implementation differs from upsert in a way that allows triggers to work.</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to merge in</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to merge in the dataframe</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replaceDataframeSchema</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">mergeInto</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.mergeIntoWithRdd"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.mergeIntoWithRdd">[docs]</a>    <span class="k">def</span> <span class="nf">mergeIntoWithRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Rows in the rdd whose primary key is not in schemaTableName will be inserted into the table;</span>
<span class="sd">        rows in the rdd whose primary key is in schemaTableName will be used to update the table.</span>

<span class="sd">        This implementation differs from upsertWithRdd in a way that allows triggers to work.</span>

<span class="sd">        :param rdd: (RDD) The RDD you would like to merge in</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) The table in which you would like to merge in the RDD</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mergeInto</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.delete"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.delete">[docs]</a>    <span class="k">def</span> <span class="nf">delete</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Delete records in a dataframe based on joining by primary keys from the data frame.</span>
<span class="sd">        Be careful with column naming and case sensitivity.</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to delete</span>
<span class="sd">        :param schema_table_name: (str) Splice Machine Table</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.deleteWithRdd"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.deleteWithRdd">[docs]</a>    <span class="k">def</span> <span class="nf">deleteWithRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Delete records using an rdd based on joining by primary keys from the rdd.</span>
<span class="sd">        Be careful with column naming and case sensitivity.</span>

<span class="sd">        :param rdd: (RDD) The RDD containing the primary keys you would like to delete from the table</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) Splice Machine Table</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delete</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.update"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update data from a dataframe for a specified schema_table_name (schema.table).</span>
<span class="sd">        The keys are required for the update and any other columns provided will be updated</span>
<span class="sd">        in the rows.</span>

<span class="sd">        :param dataframe: (Dataframe) The dataframe you would like to update</span>
<span class="sd">        :param schema_table_name: (str) Splice Machine Table</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># make sure column names are in the correct case</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">replaceDataframeSchema</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.updateWithRdd"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.updateWithRdd">[docs]</a>    <span class="k">def</span> <span class="nf">updateWithRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Update data from an rdd for a specified schema_table_name (schema.table).</span>
<span class="sd">        The keys are required for the update and any other columns provided will be updated</span>
<span class="sd">        in the rows.</span>

<span class="sd">        :param rdd: (RDD) The RDD you would like to use for updating the table</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) Splice Machine Table</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.getSchema"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.getSchema">[docs]</a>    <span class="k">def</span> <span class="nf">getSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return the schema via JDBC.</span>

<span class="sd">        :param schema_table_name: (str) Table name</span>
<span class="sd">        :return: (StructType) PySpark StructType representation of the table</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">_parse_datatype_json_string</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">getSchema</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">)</span><span class="o">.</span><span class="n">json</span><span class="p">())</span></div>

<div class="viewcode-block" id="PySpliceContext.execute"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.execute">[docs]</a>    <span class="k">def</span> <span class="nf">execute</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_string</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        execute a query over JDBC</span>

<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>
<span class="sd">            </span>
<span class="sd">                splice.execute(&#39;DELETE FROM TABLE1 WHERE col2 &gt; 3&#39;)</span>

<span class="sd">        :param query_string: (str) SQL Query (eg. SELECT * FROM table1 WHERE col2 &gt; 3)</span>
<span class="sd">        :return: None</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="n">query_string</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.executeUpdate"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.executeUpdate">[docs]</a>    <span class="k">def</span> <span class="nf">executeUpdate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_string</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        execute a dml query:(update,delete,drop,etc)</span>

<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                splice.executeUpdate(&#39;DROP TABLE table1&#39;)</span>

<span class="sd">        :param query_string: (string) SQL Query (eg. DROP TABLE table1)</span>
<span class="sd">        :return: None</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">executeUpdate</span><span class="p">(</span><span class="n">query_string</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.internalDf"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.internalDf">[docs]</a>    <span class="k">def</span> <span class="nf">internalDf</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_string</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        SQL to Dataframe translation (Lazy). Runs the query inside Splice Machine and sends the results to the Spark Adapter app</span>

<span class="sd">        :param query_string: (str) SQL Query (eg. SELECT * FROM table1 WHERE col2 &gt; 3)</span>
<span class="sd">        :return: (DataFrame) pyspark dataframe contains the result of query_string</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">return</span> <span class="n">DataFrame</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">internalDf</span><span class="p">(</span><span class="n">query_string</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_sql_context</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.rdd"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.rdd">[docs]</a>    <span class="k">def</span> <span class="nf">rdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">column_projection</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Table with projections in Splice mapped to an RDD.</span>

<span class="sd">        :param schema_table_name: (string) Accessed table</span>
<span class="sd">        :param column_projection: (list of strings) Names of selected columns</span>
<span class="sd">        :return: (RDD[Row]) the result of the projection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">column_projection</span><span class="p">:</span>
            <span class="n">colnames</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">column_projection</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">colnames</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">(</span><span class="s1">&#39;select &#39;</span><span class="o">+</span><span class="n">colnames</span><span class="o">+</span><span class="s1">&#39; from &#39;</span><span class="o">+</span><span class="n">schema_table_name</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span></div>

<div class="viewcode-block" id="PySpliceContext.internalRdd"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.internalRdd">[docs]</a>    <span class="k">def</span> <span class="nf">internalRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">column_projection</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Table with projections in Splice mapped to an RDD.</span>
<span class="sd">        Runs the projection inside Splice Machine and sends the results to the Spark Adapter app as an rdd</span>

<span class="sd">        :param schema_table_name: (str) Accessed table</span>
<span class="sd">        :param column_projection: (list of strings) Names of selected columns</span>
<span class="sd">        :return: (RDD[Row]) the result of the projection</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">column_projection</span><span class="p">:</span>
            <span class="n">colnames</span> <span class="o">=</span> <span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">column_projection</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">colnames</span> <span class="o">=</span> <span class="s1">&#39;*&#39;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">internalDf</span><span class="p">(</span><span class="s1">&#39;select &#39;</span><span class="o">+</span><span class="n">colnames</span><span class="o">+</span><span class="s1">&#39; from &#39;</span><span class="o">+</span><span class="n">schema_table_name</span><span class="p">)</span><span class="o">.</span><span class="n">rdd</span></div>

<div class="viewcode-block" id="PySpliceContext.truncateTable"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.truncateTable">[docs]</a>    <span class="k">def</span> <span class="nf">truncateTable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Truncate a table</span>

<span class="sd">        :param schema_table_name: (str) the full table name in the format &quot;schema.table_name&quot; which will be truncated</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">truncateTable</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.analyzeSchema"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.analyzeSchema">[docs]</a>    <span class="k">def</span> <span class="nf">analyzeSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Analyze the schema</span>

<span class="sd">        :param schema_name: (str) schema name which stats info will be collected</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">analyzeSchema</span><span class="p">(</span><span class="n">schema_name</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.analyzeTable"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.analyzeTable">[docs]</a>    <span class="k">def</span> <span class="nf">analyzeTable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">estimateStatistics</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">samplePercent</span><span class="o">=</span><span class="mf">10.0</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Collect stats info on a table</span>
<span class="sd">        </span>
<span class="sd">        :param schema_table_name: full table name in the format of &#39;schema.table&#39;</span>
<span class="sd">        :param estimateStatistics: will use estimate statistics if True</span>
<span class="sd">        :param samplePercent: the percentage or rows to be sampled.</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">analyzeTable</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">,</span> <span class="n">estimateStatistics</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">samplePercent</span><span class="p">))</span></div>

<div class="viewcode-block" id="PySpliceContext.export"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.export">[docs]</a>    <span class="k">def</span> <span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
               <span class="n">dataframe</span><span class="p">,</span>
               <span class="n">location</span><span class="p">,</span>
               <span class="n">compression</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
               <span class="n">replicationCount</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
               <span class="n">fileEncoding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">fieldSeparator</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">quoteCharacter</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Export a dataFrame in CSV</span>

<span class="sd">        :param dataframe: (DataFrame)</span>
<span class="sd">        :param location: (str) Destination directory</span>
<span class="sd">        :param compression: (bool) Whether to compress the output or not</span>
<span class="sd">        :param replicationCount: (int) Replication used for HDFS write</span>
<span class="sd">        :param fileEncoding: (str) fileEncoding or None, defaults to UTF-8</span>
<span class="sd">        :param fieldSeparator: (str) fieldSeparator or None, defaults to &#39;,&#39;</span>
<span class="sd">        :param quoteCharacter: (str) quoteCharacter or None, defaults to &#39;&quot;&#39;</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="n">compression</span><span class="p">,</span> <span class="n">replicationCount</span><span class="p">,</span>
                                   <span class="n">fileEncoding</span><span class="p">,</span> <span class="n">fieldSeparator</span><span class="p">,</span> <span class="n">quoteCharacter</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.exportBinary"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.exportBinary">[docs]</a>    <span class="k">def</span> <span class="nf">exportBinary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="n">compression</span><span class="p">,</span> <span class="n">e_format</span><span class="o">=</span><span class="s1">&#39;parquet&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Export a dataFrame in binary format</span>

<span class="sd">        :param dataframe: (DataFrame)</span>
<span class="sd">        :param location: (str) Destination directory</span>
<span class="sd">        :param compression: (bool) Whether to compress the output or not</span>
<span class="sd">        :param e_format: (str) Binary format to be used, currently only &#39;parquet&#39; is supported. [Default &#39;parquet&#39;]</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">exportBinary</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">location</span><span class="p">,</span> <span class="n">compression</span><span class="p">,</span> <span class="n">e_format</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.bulkImportHFile"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.bulkImportHFile">[docs]</a>    <span class="k">def</span> <span class="nf">bulkImportHFile</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Bulk Import HFile from a dataframe into a schema.table</span>

<span class="sd">        :param dataframe: (DataFrame)</span>
<span class="sd">        :param schema_table_name: (str) Full table name in the format of &quot;schema.table&quot;</span>
<span class="sd">        :param options: (Dict) Dictionary of options to be passed to --splice-properties; bulkImportDirectory is required</span>
<span class="sd">        :return: (int) Number of records imported</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">optionsMap</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">java</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">HashMap</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">options</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">optionsMap</span><span class="o">.</span><span class="n">put</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">bulkImportHFile</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">optionsMap</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.bulkImportHFileWithRdd"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.bulkImportHFileWithRdd">[docs]</a>    <span class="k">def</span> <span class="nf">bulkImportHFileWithRdd</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">options</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Bulk Import HFile from an rdd into a schema.table</span>

<span class="sd">        :param rdd: (RDD) Input data</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :param schema_table_name: (str) Full table name in the format of &quot;schema.table&quot;</span>
<span class="sd">        :param options: (Dict) Dictionary of options to be passed to --splice-properties; bulkImportDirectory is required</span>
<span class="sd">        :return:  (int) Number of records imported</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bulkImportHFile</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">),</span>
            <span class="n">schema_table_name</span><span class="p">,</span>
            <span class="n">options</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.splitAndInsert"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.splitAndInsert">[docs]</a>    <span class="k">def</span> <span class="nf">splitAndInsert</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">sample_fraction</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Sample the dataframe, split the table, and insert a dataFrame into a schema.table.</span>
<span class="sd">        This corresponds to an insert into from select statement</span>

<span class="sd">        :param dataframe: (DataFrame) Input data</span>
<span class="sd">        :param schema_table_name: (str) Full table name in the format of &quot;schema.table&quot;</span>
<span class="sd">        :param sample_fraction: (float) A value between 0 and 1 that specifies the percentage of data in the dataFrame \</span>
<span class="sd">        that should be sampled to determine the splits. \</span>
<span class="sd">        For example, specify 0.005 if you want 0.5% of the data sampled.</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">splitAndInsert</span><span class="p">(</span><span class="n">dataframe</span><span class="o">.</span><span class="n">_jdf</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="n">sample_fraction</span><span class="p">))</span></div>

<div class="viewcode-block" id="PySpliceContext.createDataFrame"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.createDataFrame">[docs]</a>    <span class="k">def</span> <span class="nf">createDataFrame</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a dataframe from a given rdd and schema.</span>

<span class="sd">        :param rdd: (RDD) Input data</span>
<span class="sd">        :param schema: (StructType) The schema of the rows in the RDD</span>
<span class="sd">        :return: (DataFrame) The Spark DataFrame</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">rdd</span><span class="p">,</span> <span class="n">schema</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_generateDBSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">types</span><span class="o">=</span><span class="p">{}):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Generate the schema for create table</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># convert keys and values to uppercase in the types dictionary</span>
        <span class="n">types</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">((</span><span class="n">key</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">types</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="n">db_schema</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># convert dataframe to have all uppercase column names</span>
        <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toUpper</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>
        <span class="c1"># i contains the name and pyspark datatype of the column</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">schema</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">i</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span> <span class="ow">in</span> <span class="n">types</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Column </span><span class="si">{}</span><span class="s1"> is of type </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">i</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">i</span><span class="o">.</span><span class="n">dataType</span><span class="p">))</span>
                <span class="n">dt</span> <span class="o">=</span> <span class="n">types</span><span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">()]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dt</span> <span class="o">=</span> <span class="n">CONVERSIONS</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">dataType</span><span class="p">)]</span>
            <span class="n">db_schema</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">i</span><span class="o">.</span><span class="n">name</span><span class="o">.</span><span class="n">upper</span><span class="p">(),</span> <span class="n">dt</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">db_schema</span>

    <span class="k">def</span> <span class="nf">_getCreateTableSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">new_schema</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parse schema for new table; if it is needed, create it</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># try to get schema and table, else set schema to splice</span>
        <span class="k">if</span> <span class="s1">&#39;.&#39;</span> <span class="ow">in</span> <span class="n">schema_table_name</span><span class="p">:</span>
            <span class="n">schema</span><span class="p">,</span> <span class="n">table</span> <span class="o">=</span> <span class="n">schema_table_name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">schema</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">getConnection</span><span class="p">()</span><span class="o">.</span><span class="n">getCurrentSchemaName</span><span class="p">()</span>
            <span class="n">table</span> <span class="o">=</span> <span class="n">schema_table_name</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        <span class="c1"># check for new schema</span>
        <span class="k">if</span> <span class="n">new_schema</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Creating schema </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">schema</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">execute</span><span class="p">(</span><span class="s1">&#39;CREATE SCHEMA </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">schema</span><span class="p">))</span>

        <span class="k">return</span> <span class="n">schema</span><span class="p">,</span> <span class="n">table</span>

    <span class="k">def</span> <span class="nf">_dropTableIfExists</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Drop table if it exists</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tableExists</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="o">=</span><span class="n">schema_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Table exists. Dropping table&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dropTable</span><span class="p">(</span><span class="n">schema_and_or_table_name</span><span class="o">=</span><span class="n">schema_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="n">table_name</span><span class="p">)</span>

<div class="viewcode-block" id="PySpliceContext.dropTableIfExists"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.dropTableIfExists">[docs]</a>    <span class="k">def</span> <span class="nf">dropTableIfExists</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Drops a table if exists</span>
<span class="sd">        </span>
<span class="sd">        :Example:</span>
<span class="sd">            .. code-block:: python</span>

<span class="sd">                splice.dropTableIfExists(&#39;schemaName.tableName&#39;) \n</span>
<span class="sd">                # or\n</span>
<span class="sd">                splice.dropTableIfExists(&#39;schemaName&#39;, &#39;tableName&#39;)</span>

<span class="sd">        :param schema_table_name: (str) Pass the schema name in this param when passing the table_name param,</span>
<span class="sd">          or pass schemaName.tableName in this param without passing the table_name param</span>
<span class="sd">        :param table_name: (optional) (str) Table Name, used when schema_table_name contains only the schema name</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_dropTableIfExists</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">,</span> <span class="n">table_name</span><span class="p">)</span></div>

    <span class="k">def</span> <span class="nf">_jstructtype</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Convert python StructType to java StructType</span>

<span class="sd">        :param schema: PySpark StructType</span>
<span class="sd">        :return: Java Spark StructType</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark_session</span><span class="o">.</span><span class="n">_jsparkSession</span><span class="o">.</span><span class="n">parseDataType</span><span class="p">(</span><span class="n">schema</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>

<div class="viewcode-block" id="PySpliceContext.createTable"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.createTable">[docs]</a>    <span class="k">def</span> <span class="nf">createTable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">primary_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">create_table_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">to_upper</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_table</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a schema.table (schema_table_name) from a dataframe</span>
<span class="sd">        </span>
<span class="sd">        :param dataframe: The Spark DataFrame to base the table off</span>
<span class="sd">        :param schema_table_name: str The schema.table to create</span>
<span class="sd">        :param primary_keys: List[str] the primary keys. Default None</span>
<span class="sd">        :param create_table_options: str The additional table-level SQL options default None</span>
<span class="sd">        :param to_upper: bool If the dataframe columns should be converted to uppercase before table creation. \</span>
<span class="sd">            If False, the table will be created with lower case columns. Default True</span>
<span class="sd">        :param drop_table: bool whether to drop the table if it exists. Default False. If False and the table exists, the function will throw an exception</span>
<span class="sd">        :return: None</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">drop_table</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_dropTableIfExists</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">to_upper</span><span class="p">:</span>
            <span class="n">dataframe</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">toUpper</span><span class="p">(</span><span class="n">dataframe</span><span class="p">)</span>
        <span class="n">primary_keys</span> <span class="o">=</span> <span class="n">primary_keys</span> <span class="k">if</span> <span class="n">primary_keys</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">createTableWithSchema</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">,</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">schema</span><span class="p">,</span>
                                   <span class="n">keys</span><span class="o">=</span><span class="n">primary_keys</span><span class="p">,</span> <span class="n">create_table_options</span><span class="o">=</span><span class="n">create_table_options</span><span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.createTableWithSchema"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.createTableWithSchema">[docs]</a>    <span class="k">def</span> <span class="nf">createTableWithSchema</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">schema</span><span class="p">,</span> <span class="n">keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">create_table_options</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a schema.table from a schema</span>

<span class="sd">        :param schema_table_name: str The schema.table to create</span>
<span class="sd">        :param schema: (StructType) The schema that describes the columns of the table</span>
<span class="sd">        :param keys: (List[str]) The primary keys. Default None</span>
<span class="sd">        :param create_table_options: (str) The additional table-level SQL options. Default None</span>
<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">keys</span><span class="p">:</span>
            <span class="n">keys_seq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">PythonUtils</span><span class="o">.</span><span class="n">toSeq</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">keys_seq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">PythonUtils</span><span class="o">.</span><span class="n">toSeq</span><span class="p">([])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">createTable</span><span class="p">(</span>
            <span class="n">schema_table_name</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_jstructtype</span><span class="p">(</span><span class="n">schema</span><span class="p">),</span>
            <span class="n">keys_seq</span><span class="p">,</span>
            <span class="n">create_table_options</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="PySpliceContext.createAndInsertTable"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.PySpliceContext.createAndInsertTable">[docs]</a>    <span class="k">def</span> <span class="nf">createAndInsertTable</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">primary_keys</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                             <span class="n">create_table_options</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">to_upper</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Creates a schema.table (schema_table_name) from a dataframe and inserts the dataframe into the table</span>

<span class="sd">        :param dataframe: The Spark DataFrame to base the table off</span>
<span class="sd">        :param schema_table_name: str The schema.table to create</span>
<span class="sd">        :param primary_keys: List[str] the primary keys. Default None</span>
<span class="sd">        :param create_table_options: str The additional table-level SQL options default None</span>
<span class="sd">        :param to_upper: bool If the dataframe columns should be converted to uppercase before table creation. \</span>
<span class="sd">            If False, the table will be created with lower case columns. Default True</span>
<span class="sd">        :param drop_table: bool whether to drop the table if it exists. Default False. If False and the table exists, the function will throw an exception</span>
<span class="sd">        :return: None</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tableExists</span><span class="p">(</span><span class="n">schema_table_name</span><span class="p">):</span>
            <span class="k">raise</span> <span class="n">SpliceMachineException</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Table </span><span class="si">{</span><span class="n">schema_table_name</span><span class="si">}</span><span class="s1"> already exists. Drop the table first or call &#39;</span>
                                         <span class="sa">f</span><span class="s1">&#39;splice.insert with the provided dataframe&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">createTable</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">primary_keys</span><span class="o">=</span><span class="n">primary_keys</span><span class="p">,</span>
                                 <span class="n">create_table_options</span><span class="o">=</span><span class="n">create_table_options</span><span class="p">,</span> <span class="n">to_upper</span><span class="o">=</span><span class="n">to_upper</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">dataframe</span><span class="p">,</span> <span class="n">schema_table_name</span><span class="p">,</span> <span class="n">to_upper</span><span class="o">=</span><span class="n">to_upper</span><span class="p">)</span></div></div>

<div class="viewcode-block" id="ExtPySpliceContext"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext">[docs]</a><span class="k">class</span> <span class="nc">ExtPySpliceContext</span><span class="p">(</span><span class="n">PySpliceContext</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This class implements a SplicemachineContext object from com.splicemachine.spark2 for use outside of the K8s Cloud Service</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_spliceSparkPackagesName</span> <span class="o">=</span> <span class="s2">&quot;com.splicemachine.spark2.splicemachine.*&quot;</span>

    <span class="k">def</span> <span class="nf">_splicemachineContext</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">jvm</span><span class="o">.</span><span class="n">com</span><span class="o">.</span><span class="n">splicemachine</span><span class="o">.</span><span class="n">spark2</span><span class="o">.</span><span class="n">splicemachine</span><span class="o">.</span><span class="n">SplicemachineContext</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">jdbcurl</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kafkaServers</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kafkaPollTimeout</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sparkSession</span><span class="p">,</span> <span class="n">JDBC_URL</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">kafkaServers</span><span class="o">=</span><span class="s1">&#39;localhost:9092&#39;</span><span class="p">,</span>
                 <span class="n">kafkaPollTimeout</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">_unit_testing</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        :param JDBC_URL: (string) The JDBC URL Connection String for your Splice Machine Cluster</span>
<span class="sd">        :param sparkSession: (sparkContext) A SparkSession object for talking to Spark</span>
<span class="sd">        :param kafkaServers (string) Comma-separated list of Kafka broker addresses in the form host:port</span>
<span class="sd">        :param kafkaPollTimeout (int) Number of milliseconds to wait when polling Kafka</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kafkaServers</span> <span class="o">=</span> <span class="n">kafkaServers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kafkaPollTimeout</span> <span class="o">=</span> <span class="n">kafkaPollTimeout</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">sparkSession</span><span class="p">,</span> <span class="n">JDBC_URL</span><span class="p">,</span> <span class="n">_unit_testing</span><span class="p">)</span>

<div class="viewcode-block" id="ExtPySpliceContext.setAutoCommitOn"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext.setAutoCommitOn">[docs]</a>    <span class="k">def</span> <span class="nf">setAutoCommitOn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Turn auto-commit on.  Auto-commit is on by default when the class is instantiated.</span>

<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">setAutoCommitOn</span><span class="p">()</span></div>

<div class="viewcode-block" id="ExtPySpliceContext.setAutoCommitOff"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext.setAutoCommitOff">[docs]</a>    <span class="k">def</span> <span class="nf">setAutoCommitOff</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Turn auto-commit off.</span>

<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">setAutoCommitOff</span><span class="p">()</span></div>

<div class="viewcode-block" id="ExtPySpliceContext.autoCommitting"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext.autoCommitting">[docs]</a>    <span class="k">def</span> <span class="nf">autoCommitting</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check whether auto-commit is on.</span>

<span class="sd">        :return: (Boolean) True if auto-commit is on.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">autoCommitting</span><span class="p">()</span></div>

<div class="viewcode-block" id="ExtPySpliceContext.transactional"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext.transactional">[docs]</a>    <span class="k">def</span> <span class="nf">transactional</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Check whether auto-commit is off.</span>

<span class="sd">        :return: (Boolean) True if auto-commit is off.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">transactional</span><span class="p">()</span></div>

<div class="viewcode-block" id="ExtPySpliceContext.commit"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext.commit">[docs]</a>    <span class="k">def</span> <span class="nf">commit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Commit the transaction.  Throws exception if auto-commit is on.</span>

<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">commit</span><span class="p">()</span></div>

<div class="viewcode-block" id="ExtPySpliceContext.rollback"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext.rollback">[docs]</a>    <span class="k">def</span> <span class="nf">rollback</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Rollback the transaction.  Throws exception if auto-commit is on.</span>

<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">rollback</span><span class="p">()</span></div>

<div class="viewcode-block" id="ExtPySpliceContext.rollbackToSavepoint"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext.rollbackToSavepoint">[docs]</a>    <span class="k">def</span> <span class="nf">rollbackToSavepoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">savepoint</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Rollback to the savepoint.  Throws exception if auto-commit is on.</span>
<span class="sd">        :param savepoint: (java.sql.Savepoint) A Savepoint.</span>

<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">rollback</span><span class="p">(</span><span class="n">savepoint</span><span class="p">)</span></div>

<div class="viewcode-block" id="ExtPySpliceContext.setSavepoint"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext.setSavepoint">[docs]</a>    <span class="k">def</span> <span class="nf">setSavepoint</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create and set a unnamed savepoint at the current point in the transaction.  Throws exception if auto-commit is on.</span>

<span class="sd">        :return: (java.sql.Savepoint) The unnamed Savepoint</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">setSavepoint</span><span class="p">()</span></div>

<div class="viewcode-block" id="ExtPySpliceContext.setSavepointWithName"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext.setSavepointWithName">[docs]</a>    <span class="k">def</span> <span class="nf">setSavepointWithName</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Create and set a named savepoint at the current point in the transaction.  Throws exception if auto-commit is on.</span>
<span class="sd">        :param name: (String) The name of the Savepoint.</span>

<span class="sd">        :return: (java.sql.Savepoint) The named Savepoint</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">setSavepoint</span><span class="p">(</span><span class="n">name</span><span class="p">)</span></div>

<div class="viewcode-block" id="ExtPySpliceContext.releaseSavepoint"><a class="viewcode-back" href="../../../splicemachine.spark.html#splicemachine.spark.context.ExtPySpliceContext.releaseSavepoint">[docs]</a>    <span class="k">def</span> <span class="nf">releaseSavepoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">savepoint</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Release the savepoint.  Throws exception if auto-commit is on.</span>
<span class="sd">        :param savepoint: (java.sql.Savepoint) A Savepoint.</span>

<span class="sd">        :return: None</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span><span class="o">.</span><span class="n">releaseSavepoint</span><span class="p">(</span><span class="n">savepoint</span><span class="p">)</span></div></div>
</pre></div>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Ben Epstein<br/>
        
            &copy; Copyright 2020, Splice Machine.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>